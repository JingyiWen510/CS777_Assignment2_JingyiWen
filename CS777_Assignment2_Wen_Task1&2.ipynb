{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS777_Assignment2_Wen_Task1&2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2RYZesMmJMO"
      },
      "source": [
        "import sys\r\n",
        "import re\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from numpy import dot\r\n",
        "from numpy.linalg import norm"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-VgKf4DnKRE"
      },
      "source": [
        "\"\"\"\r\n",
        "Task 1\r\n",
        "\"\"\"\r\n",
        "from pyspark import SparkContext\r\n",
        "sc = SparkContext ( )"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6rdw_6gmS5I"
      },
      "source": [
        "# Set the file paths on your local machine\r\n",
        "wikiPagesFile=\"gs://metcs777/WikipediaPagesOneDocPerLine1m.txt\"\r\n",
        "wikiCategoryFile=\"gs://metcs777/wiki-categorylinks.csv.bz2\"\r\n",
        "\r\n",
        "# Read files into RDDs\r\n",
        "wikiCategoryLinks=sc.textFile(wikiCategoryFile)\r\n",
        "wikiCats=wikiCategoryLinks.map(lambda x: x.split(\",\")).map(lambda x: (x[0].replace('\"',''), x[1].replace('\"','') ))\r\n",
        "wikiPages = sc.textFile(wikiPagesFile)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CSaJfKsnzq-"
      },
      "source": [
        "from pyspark.sql.session import SparkSession\r\n",
        "spark = SparkSession(sc)\r\n",
        "df = spark.read.csv(wikiPagesFile)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFB6Dd0Sn7QR",
        "outputId": "99e71f87-cf63-4f62-e75a-64b7f530ec9e"
      },
      "source": [
        "# Assumption: Each document is stored in one line of the text file\r\n",
        "# We need this count later ... \r\n",
        "numberOfDocs = wikiPages.count()\r\n",
        "# Each entry in validLines will be a line from the text file\r\n",
        "validLines = wikiPages.filter(lambda x : 'id' in x and 'url=' in x)\r\n",
        "# Now, we transform it into a set of (docID, text) pairs\r\n",
        "keyAndText = validLines.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:][:-6])) "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38e9uY6Fn_L7"
      },
      "source": [
        "def buildArray(listOfIndices):\r\n",
        "    #Build an empty array with 20000 dimension\r\n",
        "    returnVal = np.zeros(20000)\r\n",
        "    for index in listOfIndices:\r\n",
        "        returnVal[index] = returnVal[index] + 1\r\n",
        "    mysum = np.sum(returnVal)\r\n",
        "    returnVal = np.divide(returnVal, mysum)\r\n",
        "    return returnVal\r\n",
        "\r\n",
        "def build_zero_one_array (listOfIndices):\r\n",
        "    returnVal = np.zeros (20000)\r\n",
        "    for index in listOfIndices:\r\n",
        "        if returnVal[index] == 0: returnVal[index] = 1\r\n",
        "    return returnVal\r\n",
        "\r\n",
        "def stringVector(x):\r\n",
        "    returnVal = str(x[0])\r\n",
        "    for j in x[1]:\r\n",
        "        returnVal += ',' + str(j)\r\n",
        "    return returnVal\r\n",
        "\r\n",
        "def cousinSim (x,y):\r\n",
        "\tnormA = np.linalg.norm(x)\r\n",
        "\tnormB = np.linalg.norm(y)\r\n",
        "\treturn np.dot(x,y)/(normA*normB)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5DhHexLoQw7",
        "outputId": "69f70c36-1bdb-4de3-8dc2-0b398f83be9f"
      },
      "source": [
        "# Now, we split the text in each (docID, text) pair into a list of words\r\n",
        "# After this step, we have a data set with (docID, [\"word1\", \"word2\", \"word3\", ...])\r\n",
        "# We use a regular expression here to make sure that the program does not break down on some of the documents\r\n",
        "regex = re.compile('[^a-zA-Z]')\r\n",
        "# remove all non letter characters\r\n",
        "\r\n",
        "keyAndListOfWords = keyAndText.map(lambda x : (str(x[0]), regex.sub(' ', x[1]).lower().split()))\r\n",
        "# better solution here is to use NLTK tokenizer\r\n",
        "\r\n",
        "# Now get the top 20,000 words... first change (docID, [\"word1\", \"word2\", \"word3\", ...])\r\n",
        "# to (\"word1\", 1) (\"word2\", 1)...\r\n",
        "allWords = keyAndListOfWords.flatMap(lambda x: x[1]).map(lambda x: (x, 1))\r\n",
        "\r\n",
        "# Now, count all of the words, giving us (\"word1\", 1433), (\"word2\", 3423423), etc.\r\n",
        "from operator import add\r\n",
        "allCounts = allWords.reduceByKey(add)\r\n",
        "\r\n",
        "# Get the top 20,000 words in a local array in a sorted format based on frequency\r\n",
        "# If you want to run it on your laptio, it may a longer time for top 20k words. \r\n",
        "topWords = allCounts.top(20000, lambda x: x[1])\r\n",
        "\r\n",
        "print(\"Top Words in Corpus:\", allCounts.top(10, key=lambda x: x[1]))\r\n",
        "\r\n",
        "# We'll create a RDD that has a set of (word, dictNum) pairs\r\n",
        "# start by creating an RDD that has the number 0 through 20000\r\n",
        "# 20000 is the number of words that will be in our dictionary\r\n",
        "topWordsK = sc.parallelize(range(20000))\r\n",
        "# Now, we transform (0), (1), (2), ... to (\"MostCommonWord\", 1)\r\n",
        "# (\"NextMostCommon\", 2), ...\r\n",
        "# the number will be the spot in the dictionary used to tell us\r\n",
        "# where the word is located\r\n",
        "dictionary = topWordsK.map (lambda x : (topWords[x][0], x))\r\n",
        "print(\"Word Postions in our Feature Matrix. Last 20 words in 20k positions: \", dictionary.top(20, lambda x : x[1]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top Words in Corpus: [('the', 74530), ('of', 34512), ('and', 28479), ('in', 27758), ('to', 22583), ('a', 21212), ('was', 12160), ('as', 8811), ('for', 8773), ('on', 8435)]\n",
            "Word Postions in our Feature Matrix. Last 20 words in 20k positions:  [('quebecor', 19999), ('poten', 19998), ('kasada', 19997), ('yadnya', 19996), ('drift', 19995), ('iata', 19994), ('satire', 19993), ('expreso', 19992), ('olimpico', 19991), ('auxiliaries', 19990), ('tenses', 19989), ('petherick', 19988), ('stowe', 19987), ('infimum', 19986), ('parramatta', 19985), ('rimpac', 19984), ('hyderabad', 19983), ('cubes', 19982), ('meats', 19981), ('chaat', 19980)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4h0sBFtonjV",
        "outputId": "68edff60-973a-4aab-c7d0-20e3d997de61"
      },
      "source": [
        "################### TASK 2  ##################\r\n",
        "\r\n",
        "# Next, we get a RDD that has, for each (docID, [\"word1\", \"word2\", \"word3\", ...]),\r\n",
        "# (\"word1\", docID), (\"word2\", docId), ...\r\n",
        "\r\n",
        "allWordsWithDocID = keyAndListOfWords.flatMap(lambda x: ((j, x[0]) for j in x[1]))\r\n",
        "\r\n",
        "\r\n",
        "# Now join and link them, to get a set of (\"word1\", (dictionaryPos, docID)) pairs\r\n",
        "allDictionaryWords = dictionary.join(allWordsWithDocID)\r\n",
        "\r\n",
        "# Now, we drop the actual word itself to get a set of (docID, dictionaryPos) pairs\r\n",
        "justDocAndPos = allDictionaryWords.map(lambda x:(x[1][1],x[1][0]))\r\n",
        "\r\n",
        "\r\n",
        "# Now get a set of (docID, [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\r\n",
        "allDictionaryWordsInEachDoc = justDocAndPos.groupByKey()\r\n",
        "\r\n",
        "\r\n",
        "# The following line this gets us a set of\r\n",
        "# (docID,  [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\r\n",
        "# and converts the dictionary positions to a bag-of-words numpy array...\r\n",
        "allDocsAsNumpyArrays = allDictionaryWordsInEachDoc.map(lambda x: (x[0], buildArray(x[1])))\r\n",
        "\r\n",
        "print(allDocsAsNumpyArrays.take(3))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('431952', array([0.08850932, 0.02950311, 0.04658385, ..., 0.        , 0.        ,\n",
            "       0.        ])), ('431971', array([0.08553655, 0.02488336, 0.05132193, ..., 0.        , 0.        ,\n",
            "       0.        ])), ('431989', array([0.09074244, 0.03574702, 0.03849679, ..., 0.        , 0.        ,\n",
            "       0.        ]))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCqf0JpforBb",
        "outputId": "327b8aff-266c-4f67-926d-b5cd483550a4"
      },
      "source": [
        "# Now, create a version of allDocsAsNumpyArrays where, in the array,\r\n",
        "# every entry is either zero or one.\r\n",
        "# A zero means that the word does not occur,\r\n",
        "# and a one means that it does.\r\n",
        "\r\n",
        "zeroOrOne = allDocsAsNumpyArrays.map(lambda x: (x[0],np.where(x[1]!=0,1,0)))\r\n",
        "\r\n",
        "# Now, add up all of those arrays into a single array, where the\r\n",
        "# i^th entry tells us how many\r\n",
        "# individual documents the i^th word in the dictionary appeared in\r\n",
        "dfArray = zeroOrOne.reduce(lambda x1, x2: (\"\", np.add(x1[1], x2[1])))[1]\r\n",
        "\r\n",
        "# Create an array of 20,000 entries, each entry with the value numberOfDocs (number of docs)\r\n",
        "multiplier = np.full(20000, numberOfDocs)\r\n",
        "\r\n",
        "# Get the version of dfArray where the i^th entry is the inverse-document frequency for the\r\n",
        "# i^th word in the corpus\r\n",
        "idfArray = np.log(np.divide(np.full(20000, numberOfDocs),dfArray))\r\n",
        "\r\n",
        "# Finally, convert all of the tf vectors in allDocsAsNumpyArrays to tf * idf vectors\r\n",
        "allDocsAsNumpyArraysTFidf = allDocsAsNumpyArrays.map(lambda x: (x[0], np.multiply(x[1], idfArray)))\r\n",
        "# use the buildArray function to build the feature array\r\n",
        "# allDocsAsNumpyArrays = allDictionaryWordsInEachDoc.map(lambda x: (x[0], buildArray(x[1])))\r\n",
        "\r\n",
        "print(allDocsAsNumpyArraysTFidf.take(2))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('431952', array([0.00747631, 0.00246002, 0.00654094, ..., 0.        , 0.        ,\n",
            "       0.        ])), ('431971', array([0.0072252 , 0.00207481, 0.00720622, ..., 0.        , 0.        ,\n",
            "       0.        ]))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IrTgKNOoyUD",
        "outputId": "04ae4bec-55e7-4564-d52f-14fd079550f4"
      },
      "source": [
        "# Now, we join it with categories, and map it after join so that we have only the wikipageID \r\n",
        "# This joun can take time on your laptop. \r\n",
        "# You can do the join once and generate a new wikiCats data and store it. Our WikiCategories includes all categories\r\n",
        "# of wikipedia. \r\n",
        "\r\n",
        "featuresRDD = wikiCats.join(allDocsAsNumpyArraysTFidf).map(lambda x: (x[1][0], x[1][1]))\r\n",
        "\r\n",
        "# Cache this important data because we need to run kNN on this data set. \r\n",
        "featuresRDD.cache()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('All_articles_with_unsourced_statements',\n",
              "  array([0.00619833, 0.00281363, 0.00376037, ..., 0.        , 0.        ,\n",
              "         0.        ])),\n",
              " ('Articles_with_inconsistent_citation_formats',\n",
              "  array([0.00619833, 0.00281363, 0.00376037, ..., 0.        , 0.        ,\n",
              "         0.        ])),\n",
              " ('Articles_with_unsourced_statements_from_February_2013',\n",
              "  array([0.00619833, 0.00281363, 0.00376037, ..., 0.        , 0.        ,\n",
              "         0.        ])),\n",
              " ('CS1:_long_volume_value',\n",
              "  array([0.00619833, 0.00281363, 0.00376037, ..., 0.        , 0.        ,\n",
              "         0.        ])),\n",
              " ('CS1_maint:_archived_copy_as_title',\n",
              "  array([0.00619833, 0.00281363, 0.00376037, ..., 0.        , 0.        ,\n",
              "         0.        ])),\n",
              " ('Continued_fractions',\n",
              "  array([0.00619833, 0.00281363, 0.00376037, ..., 0.        , 0.        ,\n",
              "         0.        ])),\n",
              " ('Gamma_and_related_functions',\n",
              "  array([0.00619833, 0.00281363, 0.00376037, ..., 0.        , 0.        ,\n",
              "         0.        ])),\n",
              " ('CS1_Spanish-language_sources_(es)',\n",
              "  array([0.00637503, 0.00943943, 0.00794786, ..., 0.        , 0.        ,\n",
              "         0.        ])),\n",
              " ('CS1_maint:_archived_copy_as_title',\n",
              "  array([0.00637503, 0.00943943, 0.00794786, ..., 0.        , 0.        ,\n",
              "         0.        ])),\n",
              " ('Lists_of_states_of_Mexico',\n",
              "  array([0.00637503, 0.00943943, 0.00794786, ..., 0.        , 0.        ,\n",
              "         0.        ]))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7OLFpRio2hb"
      },
      "source": [
        "# Finally, we have a function that returns the prediction for the label of a string, using a kNN algorithm\r\n",
        "def getPrediction (textInput, k):\r\n",
        "    # Create an RDD out of the textIput\r\n",
        "    myDoc = sc.parallelize (('', textInput))\r\n",
        "\r\n",
        "    # Flat map the text to (word, 1) pair for each word in the doc\r\n",
        "    wordsInThatDoc = myDoc.flatMap (lambda x : ((j, 1) for j in regex.sub(' ', x).lower().split()))\r\n",
        "\r\n",
        "    # This will give us a set of (word, (dictionaryPos, 1)) pairs\r\n",
        "    allDictionaryWordsInThatDoc = dictionary.join (wordsInThatDoc).map (lambda x: (x[1][1], x[1][0])).groupByKey ()\r\n",
        "\r\n",
        "    # Get tf array for the input string\r\n",
        "    myArray = buildArray (allDictionaryWordsInThatDoc.top (1)[0][1])\r\n",
        "\r\n",
        "    # Get the tf * idf array for the input string\r\n",
        "    myArray = np.multiply (dfArray,idfArray)\r\n",
        "\r\n",
        "    # Get the distance from the input text string to all database documents, using cosine similarity (np.dot() )\r\n",
        "    distances = featuresRDD.map (lambda x : (x[0], np.dot (x[1], myArray)))\r\n",
        "    # distances = allDocsAsNumpyArraysTFidf.map (lambda x : (x[0], cousinSim (x[1],myArray)))\r\n",
        "    # get the top k distances\r\n",
        "    topK = distances.top (k, lambda x : x[1])\r\n",
        "    \r\n",
        "    # and transform the top k distances into a set of (docID, 1) pairs\r\n",
        "    docIDRepresented = sc.parallelize(topK).map (lambda x : (x[0], 1))\r\n",
        "\r\n",
        "    # now, for each docID, get the count of the number of times this document ID appeared in the top k\r\n",
        "    numTimes = docIDRepresented.reduceByKey(lambda x, y: x+y)\r\n",
        "    \r\n",
        "    # Return the top 1 of them.\r\n",
        "    # Ask yourself: Why we are using twice top() operation here?\r\n",
        "    return numTimes.top(k, lambda x: x[1])"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dF7QOvHOo9aW",
        "outputId": "ff73f32f-5cf0-43a9-9fca-9d3675400dd4"
      },
      "source": [
        "print(getPrediction('Sport Basketball Volleyball Soccer', 10))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Disambiguation_pages_with_short_description', 2), ('All_article_disambiguation_pages', 2), ('All_disambiguation_pages', 2), ('Military_units_and_formations_disambiguation_pages', 2), ('All_set_index_articles', 1), ('Articles_with_short_description', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CGhZJjeo_5p",
        "outputId": "0e0400a2-d185-4944-9487-e2e70cfc10f5"
      },
      "source": [
        "print(getPrediction('What is the capital city of Australia?', 10))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Disambiguation_pages_with_short_description', 2), ('All_article_disambiguation_pages', 2), ('All_disambiguation_pages', 2), ('Military_units_and_formations_disambiguation_pages', 2), ('All_set_index_articles', 1), ('Articles_with_short_description', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTxbl9E8pCiB",
        "outputId": "697ecdac-a713-43d1-a0b5-eb27ad430dec"
      },
      "source": [
        "print(getPrediction('How many goals Vancouver score last year?', 10))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Disambiguation_pages_with_short_description', 2), ('All_article_disambiguation_pages', 2), ('All_disambiguation_pages', 2), ('Military_units_and_formations_disambiguation_pages', 2), ('All_set_index_articles', 1), ('Articles_with_short_description', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzBBP28jl96u"
      },
      "source": [
        "\r\n",
        "\r\n",
        "# Congradulations, you have implemented a prediction system based on Wikipedia data. \r\n",
        "# You can use this system to generate automated Tags or Categories for any kind of text \r\n",
        "# that you put in your query.\r\n",
        "# This data model can predict categories for any input text. \r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}