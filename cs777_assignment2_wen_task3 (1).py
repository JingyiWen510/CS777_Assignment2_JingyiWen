# -*- coding: utf-8 -*-
"""CS777_Assignment2_Wen_task3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UtZIwD294mU36_XtccAM9tGOGgwnNLbE
"""

import sys
from pyspark import SparkContext

sc = SparkContext ( )

from pyspark.sql.types import *
from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)

wikiCategoryFile="gs://metcs777/wiki-categorylinks.csv.bz2"
wikiCategoryLinks=sc.textFile(wikiCategoryFile)
wikiCats=wikiCategoryLinks.map(lambda x: x.split(",")).map(lambda x: (x[0].replace('"',''), x[1].replace('"','') ))
df=sqlContext.createDataFrame(wikiCats)

df.show()

"""
task3.1
"""
from pyspark.sql import functions as func
from pyspark.sql import DataFrameStatFunctions as statFunc
from pyspark.sql import functions as F
from pyspark.sql.functions import mean as _mean, stddev as _stddev, col
df_cate = df.groupBy (df[1]).count()
max = df_cate.agg(func.max("count")).show()
avg = df_cate.agg(func.mean("count")).show()
med = F.expr('percentile_approx(count, 0.5)')
median = df_cate.agg(med.alias('med(count)')).show()
std = df_cate.select(_stddev(col('count')).alias('std')).show()

"""
task3.2
"""
top = df_cate.orderBy("count",ascending=[0]).show(10)

"""
task3.3
"""
top_cate = df_cate.orderBy("count",ascending=[0]).limit(10)
top_page = top_cate.join(df,df[1]==top_cate[0]).drop("count")
top_id = top_page.groupBy(top_page[1]).count()
print(top_id.select(top_id[0]).show())